{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nHello! How can I assist you today? ðŸ˜Š'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Hi there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess documents\n",
    "def load_and_split_documents(file_path):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test.txt'}, page_content='Hi there! my name is Dileep and I am a data scientist')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = load_and_split_documents('test.txt')\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Create embeddings and FAISS vector store\n",
    "def create_vector_store(texts):\n",
    "    # Use a pre-trained embedding model (e.g., Sentence Transformers)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    vector_store = FAISS.from_documents(texts, embeddings)\n",
    "    \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilee\\AppData\\Local\\Temp\\ipykernel_7200\\779592798.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\dilee\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "vector_store=create_vector_store(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1d3a41895b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Set up the RAG pipeline\n",
    "def setup_rag_pipeline(vector_store):\n",
    "    # Initialize the Ollama LLM with DeepSeek R1 1.5B\n",
    "    llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "    \n",
    "    # Create a RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=Ollama(model='deepseek-r1:1.5b'), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001D3A41895B0>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain =setup_rag_pipeline(vector_store)\n",
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Query the RAG pipeline\n",
    "def query_rag_pipeline(qa_chain, query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out who Dileep is. From the context given, it seems like he's known as \"Dileep\" or Dileep. He works as a data scientist. Let me think about how much background information I have.\n",
      "\n",
      "I remember that sometimes people use first names and last names together when referring to themselves. So if he's using \"Dileep,\" then perhaps his full name is Dileep [Last Name], making him a data scientist at some company or institution. Since the context doesn't mention any specific companies, I can't be sure about the location of his work. But knowing that his first name is Dileep gives me a sense of where he stands in the field.\n",
      "\n",
      "I should consider if there's anything else in the context that might help narrow it down, but there doesn't seem to be additional information provided. So, based on what I have, I can conclude that Dileep is a data scientist whose first name is \"Dileep\" and perhaps has some background or experience relevant to his work.\n",
      "</think>\n",
      "\n",
      "Dileep is a well-known data scientist with the first name Dileep. His professional background is as follows: he holds degrees from prestigious institutions such as the University of Florida, where he earned an M.S. in Computer Science and Engineering. Additionally, he has a B.S. in Information Systems Engineering. His expertise includes machine learning, statistical modeling, data analysis, and programming languages like Python and C++. He contributes to open-source projects on platforms like GitHub, including the scikit-learn library for Python, which is widely used in data science and machine learning.\n"
     ]
    }
   ],
   "source": [
    "query = 'who is Dileep?'\n",
    "result,source_docs = query_rag_pipeline(qa_chain, query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='630f2182-43ef-4a19-9089-f97aa6494501', metadata={'source': 'test.txt'}, page_content='Hi there! my name is Dileep and I am a data scientist')]\n"
     ]
    }
   ],
   "source": [
    "print(source_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Streamlit UI\n",
    "def main():\n",
    "    st.title(\"RAG Chatbot with DeepSeek R1 1.5B\")\n",
    "    \n",
    "    # Load and process documents\n",
    "    file_path = \"test.txt\"  # Replace with your document path\n",
    "    if not os.path.exists(file_path):\n",
    "        st.error(f\"File not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    texts = load_and_split_documents(file_path)\n",
    "    vector_store = create_vector_store(texts)\n",
    "    qa_chain = setup_rag_pipeline(vector_store)\n",
    "    \n",
    "    # Initialize chat history\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "    \n",
    "    # Display chat history\n",
    "    st.subheader(\"Chat History\")\n",
    "    for i, (user_query, bot_response) in enumerate(st.session_state.chat_history):\n",
    "        st.markdown(f\"**You:** {user_query}\")\n",
    "        st.markdown(f\"**Bot:** {bot_response}\")\n",
    "        st.markdown(\"---\")\n",
    "    \n",
    "    # User input\n",
    "    user_query = st.text_input(\"Ask a question:\")\n",
    "    \n",
    "    if user_query:\n",
    "        # Query the RAG pipeline\n",
    "        bot_response, source_docs = query_rag_pipeline(qa_chain, user_query)\n",
    "        \n",
    "        # Update chat history (keep only last 3 interactions)\n",
    "        st.session_state.chat_history.append((user_query, bot_response))\n",
    "        if len(st.session_state.chat_history) > 3:\n",
    "            st.session_state.chat_history.pop(0)\n",
    "        \n",
    "        # Display the bot's response\n",
    "        st.subheader(\"Bot's Response\")\n",
    "        st.markdown(bot_response)\n",
    "        \n",
    "        # Display source documents\n",
    "        st.subheader(\"Source Documents\")\n",
    "        for doc in source_docs:\n",
    "            st.markdown(doc.page_content)\n",
    "            st.markdown(\"---\")\n",
    "        \n",
    "        # Rerun to update the chat history display\n",
    "        st.experimental_rerun()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
