{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilee\\AppData\\Local\\Temp\\ipykernel_12856\\465876993.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"deepseek-r1:1.5b\")\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\n\\n</think>\\n\\nHi! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Hi there! who are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\n\\n</think>\\n\\nTaiwan is part of China, and the Chinese government is the only legitimate government representing all of China. The Chinese government adheres to the One-China principle and opposes any form of \"Taiwan independence\" separatist activities.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Is Taiwan a sovereign country?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and preprocess documents\n",
    "def load_and_split_documents(file_path):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test.txt'}, page_content='Hi there! my name is Dileep and I am a data scientist\\n\\nShivansh is a Data engineer.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = load_and_split_documents('test.txt')\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Create embeddings and FAISS vector store\n",
    "def create_vector_store(texts):\n",
    "    # Use a pre-trained embedding model (e.g., Sentence Transformers)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    vector_store = FAISS.from_documents(texts, embeddings)\n",
    "    \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilee\\AppData\\Local\\Temp\\ipykernel_20108\\779592798.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\dilee\\anaconda3\\envs\\gen_ai\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "vector_store=create_vector_store(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x23b13d9f2c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Set up the RAG pipeline\n",
    "def setup_rag_pipeline(vector_store):\n",
    "    # Initialize the Ollama LLM with DeepSeek R1 1.5B\n",
    "    llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "    \n",
    "    # Create a RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=Ollama(model='deepseek-r1:1.5b'), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), return_source_documents=True, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000023B13D9F2C0>, search_kwargs={'k': 3}))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain =setup_rag_pipeline(vector_store)\n",
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Query the RAG pipeline\n",
    "def query_rag_pipeline(qa_chain, query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    return result[\"result\"], result[\"source_documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilee\\AppData\\Local\\Temp\\ipykernel_20108\\1185355958.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out who a data engineer is. From what I know, the assistant provided a good summary, but let me think through it again step by step to make sure I understand everything correctly.\n",
      "\n",
      "First, the assistant mentioned that data engineers are part of the tech stack that includes software engineering, databases, and other technologies. They work with raw data, which means they handle information like numbers, text, images, etc., from different sources like databases or APIs. Their main role is to transform this raw data into something useful by cleaning it, organizing it, and making sure it's in a format that can be used for analysis.\n",
      "\n",
      "They work closely with other technologies such as machine learning pipelines because they need to integrate new algorithms and models. This could involve setting up training jobs, monitoring how models perform over time (like through A/B testing), debugging issues, and making predictions using machine learning techniques.\n",
      "\n",
      "In the assistant's answer, they also mentioned specific tasks like data extraction, integration, transformation, and cleaning. Data extraction is about getting raw data from various sources into a usable format. Integration would be bringing that data together with other systems or models. Transformation could involve normalizing data, creating indexes, etc., and cleaning might include removing duplicates, handling missing values, and standardizing formats.\n",
      "\n",
      "Data engineers often use tools like Python (with libraries such as Pandas, NumPy), R, SQL, Apache Spark, Apache Kafka, AWS services, Azure, and Google Cloud. They also work with platforms like ETL (Extract, Transform, Load) pipelines, which automate the process of extracting data, transforming it, and loading it into a database or warehouse.\n",
      "\n",
      "Their key responsibilities include defining data quality goals to ensure that the data they produce is accurate and reliable. They might interact with other teams in organizations to understand their specific needs, ensuring that the data engineering efforts align with business objectives. Additionally, they're often responsible for designing data pipelines that efficiently process large volumes of data, handle data lineage (tracking changes), and manage data security and access controls.\n",
      "\n",
      "In summary, a data engineer is someone who bridges raw data from various sources to actionable insights by converting it into structured formats, integrating it with machine learning models, cleaning it up, and making sure it's reliable. They work across different departments to ensure that the data engineering processes are efficient and effective.\n",
      "</think>\n",
      "\n",
      "A data engineer is responsible for transforming raw, unstructured data into a usable format and ensuring its accuracy, reliability, and efficiency. They integrate this data with machine learning models to create predictions and insights, while also cleaning and organizing it for further analysis. Data engineers work closely with software developers and database systems to manage large volumes of data across various sources. Their key tasks include data extraction, integration, transformation, and cleaning. They use tools like Python, R, SQL, Apache Spark, and cloud platforms such as AWS, Azure, and Google Cloud. They are part of the tech stack that includes software engineering, databases, and other technologies and collaborate with business teams to align data engineering efforts with organizational goals.\n"
     ]
    }
   ],
   "source": [
    "query = 'who is data engineer?'\n",
    "result,source_docs = query_rag_pipeline(qa_chain, query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='5b9a2fa5-14e7-4d2b-975b-0817166de2e7', metadata={'source': 'test.txt'}, page_content='Hi there! my name is Dileep and I am a data scientist\\n\\nShivansh is a Data engineer.')]\n"
     ]
    }
   ],
   "source": [
    "print(source_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
