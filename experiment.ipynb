{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arxiv\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain, create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.llms import OpenLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a detailed summary of the following research paper:\n",
    "\"{text}\"\n",
    "DETAILED SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers(query, max_results=5):\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    papers = []\n",
    "    for result in search.results():\n",
    "        papers.append({\n",
    "            'title': result.title,\n",
    "            'summary': result.summary,\n",
    "            'pdf_url': result.pdf_url,\n",
    "            'published': result.published\n",
    "        })\n",
    "    return papers\n",
    "\n",
    "# Function to summarize a paper\n",
    "def summarize_paper(paper):\n",
    "    # Create a LangChain Document object\n",
    "    doc = Document(page_content=paper['summary'], metadata={\"title\": paper['title']})\n",
    "    # Generate summary\n",
    "    summary = stuff_chain.run([doc])  # Pass a list of Document objects\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter your search query for arXiv (e.g., 'AI', 'Machine Learning'):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilee\\AppData\\Local\\Temp\\ipykernel_15228\\3482828095.py:8: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    }
   ],
   "source": [
    "papers = fetch_papers(query, max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'PyEvalAI: AI-assisted evaluation of Jupyter Notebooks for immediate personalized feedback',\n",
       "  'summary': 'Grading student assignments in STEM courses is a laborious and repetitive\\ntask for tutors, often requiring a week to assess an entire class. For\\nstudents, this delay of feedback prevents iterating on incorrect solutions,\\nhampers learning, and increases stress when exercise scores determine admission\\nto the final exam. Recent advances in AI-assisted education, such as automated\\ngrading and tutoring systems, aim to address these challenges by providing\\nimmediate feedback and reducing grading workload. However, existing solutions\\noften fall short due to privacy concerns, reliance on proprietary closed-source\\nmodels, lack of support for combining Markdown, LaTeX and Python code, or\\nexcluding course tutors from the grading process. To overcome these\\nlimitations, we introduce PyEvalAI, an AI-assisted evaluation system, which\\nautomatically scores Jupyter notebooks using a combination of unit tests and a\\nlocally hosted language model to preserve privacy. Our approach is free,\\nopen-source, and ensures tutors maintain full control over the grading process.\\nA case study demonstrates its effectiveness in improving feedback speed and\\ngrading efficiency for exercises in a university-level course on numerics.',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2502.18425v1',\n",
       "  'published': datetime.datetime(2025, 2, 25, 18, 20, 20, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'TSKANMixer: Kolmogorov-Arnold Networks with MLP-Mixer Model for Time Series Forecasting',\n",
       "  'summary': 'Time series forecasting has long been a focus of research across diverse\\nfields, including economics, energy, healthcare, and traffic management. Recent\\nworks have introduced innovative architectures for time series models, such as\\nthe Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs)\\nto enhance prediction accuracy by effectively capturing both spatial and\\ntemporal dependencies within the data. In this paper, we investigate the\\ncapabilities of the Kolmogorov-Arnold Networks (KANs) for time-series\\nforecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimental\\nresults demonstrate that TSKANMixer tends to improve prediction accuracy over\\nthe original TSMixer across multiple datasets, ranking among the top-performing\\nmodels compared to other time series approaches. Our results show that the KANs\\nare promising alternatives to improve the performance of time series\\nforecasting by replacing or extending traditional MLPs.',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2502.18410v1',\n",
       "  'published': datetime.datetime(2025, 2, 25, 18, 4, 45, tzinfo=datetime.timezone.utc)},\n",
       " {'title': 'The Gradient of Algebraic Model Counting',\n",
       "  'summary': 'Algebraic model counting unifies many inference tasks on logic formulas by\\nexploiting semirings. Rather than focusing on inference, we consider learning,\\nespecially in statistical-relational and neurosymbolic AI, which combine\\nlogical, probabilistic and neural representations. Concretely, we show that the\\nvery same semiring perspective of algebraic model counting also applies to\\nlearning. This allows us to unify various learning algorithms by generalizing\\ngradients and backpropagation to different semirings. Furthermore, we show how\\ncancellation and ordering properties of a semiring can be exploited for more\\nmemory-efficient backpropagation. This allows us to obtain some interesting\\nvariations of state-of-the-art gradient-based optimisation methods for\\nprobabilistic logical models. We also discuss why algebraic model counting on\\ntractable circuits does not lead to more efficient second-order optimization.\\nEmpirically, our algebraic backpropagation exhibits considerable speed-ups as\\ncompared to existing approaches.',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2502.18406v1',\n",
       "  'published': datetime.datetime(2025, 2, 25, 17, 57, 55, tzinfo=datetime.timezone.utc)}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to write a detailed summary of this research paper. Let me read through it first.\n",
      "\n",
      "The paper is about grading student assignments in STEM courses. It says that graders spend a lot of time and time-consuming tasks, maybe even a week for each class. For students, this delay makes it hard to keep up with incorrect solutions because they can't iterate on them. This also hinders learning and increases stress when their scores determine if they get to the final exam.\n",
      "\n",
      "Then, there's mention of AI-assisted education, like automated grading systems. These systems aim to provide immediate feedback but currently have limitations. The main issues are privacy concerns—students might share too much information. Also, these solutions often use proprietary models which aren't open or accessible. They don't support Markdown, LaTeX, and Python code, so it's hard for graders to input their work properly. Plus, they exclude tutors from the grading process, meaning graders can't see how students are doing.\n",
      "\n",
      "To solve these problems, the paper introduces PyEvalAI—a new system that uses Jupyter notebooks with unit tests and a local language model. This way, graders can score assignments without compromising privacy because the model is hosted locally. It's free, open-source, and allows tutors to still grade effectively since they can see their students' work.\n",
      "\n",
      "Finally, there's a case study showing how PyEvalAI improves feedback speed and efficiency in a numerics course. The paper concludes that this system addresses the main issues mentioned earlier.\n",
      "\n",
      "Alright, so I need to make sure my summary covers all these points clearly and concisely. I should start by introducing the problem with grading being time-consuming and inefficient for both graders and students. Then explain AI solutions but their limitations. Introduce PyEvalAI as a solution that addresses those issues. Finally, present the case study and its benefits.\n",
      "\n",
      "I need to make sure each section flows logically into the next, highlighting the main points without getting too detailed in every part. Also, I should use clear language so it's easy for someone who isn't familiar with all the technical terms mentioned.\n",
      "</think>\n",
      "\n",
      "**Summary of Research Paper: AI-Assisted Evaluation in STEM Education**\n",
      "\n",
      "The paper addresses a critical issue in STEM education where grading student assignments is time-consuming and repetitive, often requiring weeks to complete. This inefficiency leads to students being unable to iterate on incorrect solutions, hindering learning and increasing stress when their scores determine final exam access.\n",
      "\n",
      "AI-assisted education systems, such as automated grading tools, aim to alleviate these challenges but face several limitations. These include privacy concerns, reliance on proprietary models, lack of support for Markdown, LaTeX, and Python code, and exclusion of tutors from the grading process. This setup disrupts the workflow for both graders and students.\n",
      "\n",
      "To overcome these limitations, the paper introduces PyEvalAI—a novel system that automates grading using Jupyter notebooks with unit tests and a local language model. This approach preserves privacy while allowing graders to grade effectively. It is free, open-source, and supports tutors in maintaining control over the grading process.\n",
      "\n",
      "A case study demonstrates PyEvalAI's effectiveness in improving feedback speed and efficiency for numerics assignments. The system successfully addresses the challenges posed by existing solutions, offering a more efficient and secure grading method that aligns with educational goals.\n",
      "==========================\n",
      "<think>\n",
      "Okay, so I need to write a detailed summary of this research paper. Let me start by reading through the abstract first to get an overview.\n",
      "\n",
      "The abstract mentions that time series forecasting has been a big area of study in various fields like economics, energy, healthcare, and traffic management. Recent work with models like TSMixer and TSKANMixer shows improved accuracy over traditional methods. The key point is that KANs are promising alternatives for enhancing time series forecasting.\n",
      "\n",
      "Hmm, I should make sure to include all the main points from the abstract: what each model does, how they improve prediction accuracy, and why KANs are better than MLPs in this context.\n",
      "\n",
      "Wait, the user provided a detailed summary already. But since I'm supposed to write my own, maybe I can think through it more carefully. Let me outline the structure:\n",
      "\n",
      "1. Introduction: Briefly mention time series forecasting's importance across different fields.\n",
      "2. Overview of TSMixer and its benefits.\n",
      "3. The problem with traditional MLPs in time series.\n",
      "4. Introducing KANs as a replacement or extension for MLPs.\n",
      "5. How TSKANMixer combines TSMixer with KANs to improve accuracy.\n",
      "6. Experimental results showing improved performance over other methods.\n",
      "7. Conclusion: Highlight the significance of using KANs for better forecasting.\n",
      "\n",
      "I should make sure each section flows logically and covers all key points without getting too detailed in every aspect. Also, I need to ensure that I'm not just summarizing but actually understanding the research thoroughly so I can explain it clearly when writing the summary.\n",
      "</think>\n",
      "\n",
      "**Time Series Forecasting: A Comprehensive Overview**\n",
      "\n",
      "In recent years, time series forecasting has emerged as a critical area of study across various disciplines such as economics, energy, healthcare, and traffic management. This field focuses on predicting future events based on historical data, which is essential for decision-making in these diverse sectors.\n",
      "\n",
      "The paper under discussion introduces two innovative models: TSMixer and TSKANMixer. TSMixer leverages multi-layer perceptrons (MLPs) to enhance prediction accuracy by capturing both spatial and temporal dependencies within the data. On the other hand, TSKANMixer combines TSMixer with KANs (Kolmogorov-Arnold Networks), which offer a promising alternative for improving MLPs in time series forecasting.\n",
      "\n",
      "The study demonstrates that TSKANMixer outperforms traditional methods across multiple datasets, ranking among top-performing models. This suggests that the integration of KANs into TSMixer not only enhances accuracy but also provides a versatile framework for future advancements in time series forecasting.\n",
      "\n",
      "In conclusion, the research highlights the potential of KANs as a valuable tool for improving time series forecasting by replacing or extending traditional MLPs, offering significant benefits for various applications.\n",
      "==========================\n",
      "<think>\n",
      "Okay, so I need to write a detailed summary of this research paper. Let me start by reading through the abstract first to get an overview.\n",
      "\n",
      "The abstract mentions that algebraic model counting unifies many inference tasks on logic formulas using semirings. Instead of focusing on inference, they're looking at learning in statistical-relational and neurosymbolic AI, which combine logical, probabilistic, and neural representations. They show that the same semiring perspective applies to learning, allowing them to unify various learning algorithms by generalizing gradients and backpropagation to different semirings. They also mention how cancellation and ordering properties of a semiring can be used for more memory-efficient backpropagation. Additionally, they discuss why algebraic model counting on tractable circuits doesn't lead to more efficient second-order optimization. Finally, they talk about empirical results showing that their algebraic backpropagation is significantly faster than existing methods.\n",
      "\n",
      "Now, I need to break this down into key points and structure the summary accordingly. The user provided a detailed summary already, but I should think through how to approach writing it myself.\n",
      "\n",
      "First, I'll identify the main components of the paper:\n",
      "\n",
      "1. **Algebraic Model Counting**: This is the core concept that unifies inference tasks using semirings.\n",
      "2. **Learning in AI**: They're focusing on learning within statistical-relational and neurosymbolic AI frameworks.\n",
      "3. **Semiring Perspective for Learning**: The idea that semiring properties can be used to generalize backpropagation algorithms.\n",
      "4. **Tractable Circuits and Second-Order Optimization**: Discussing why model counting on tractable circuits doesn't offer better second-order optimization.\n",
      "5. **Empirical Results**: Showing that algebraic backpropagation is faster than existing methods.\n",
      "\n",
      "I should make sure each of these points is covered in the summary, explaining how they connect to each other and what the implications are for the field.\n",
      "\n",
      "I also need to consider the audience. Since this is a research paper aimed at AI and machine learning, I'll assume some technical knowledge but aim to be clear enough that someone without prior reading can understand the key ideas.\n",
      "\n",
      "Let me outline the structure:\n",
      "\n",
      "- **Introduction**: Briefly mention algebraic model counting and its role in unifying inference tasks.\n",
      "- **Learning in AI**: Explain how they're applying semirings to learning, especially in statistical-relational and neurosymbolic AI.\n",
      "- **Semiring Generalization of Backpropagation**: Detail how cancellation and ordering properties are used for efficient backpropagation.\n",
      "- **Tractable Circuits and Second-Order Optimization**: Discuss the limitations of model counting on tractable circuits and why second-order methods aren't better here.\n",
      "- **Empirical Results**: Summarize the speed improvements from algebraic backpropagation compared to existing methods.\n",
      "\n",
      "I should make sure each section flows logically, connecting the concepts smoothly. I'll need to define terms like semirings, tractable models, and second-order optimization if necessary, but perhaps it's better to assume some familiarity with these areas.\n",
      "\n",
      "I also notice that the abstract mentions \"tractable circuits\" leading to no benefits for second-order methods. This is a key point because it shows a limitation of model counting in this context, which can guide future research or applications.\n",
      "\n",
      "Now, I'll start drafting each section, making sure to cover all points and explain them clearly.\n",
      "</think>\n",
      "\n",
      "**Algebraic Backpropagation: A Unified Approach to Learning in AI**\n",
      "\n",
      "In the realm of artificial intelligence, algebraic model counting has emerged as a powerful framework that unifies various inference tasks on logic formulas by leveraging semirings. This innovative approach not only streamlines traditional methods but also opens new avenues for learning within statistical-relational and neurosymbolic AI systems. By focusing on learning in these complex domains, the paper demonstrates how semiring properties can be generalized to enhance backpropagation algorithms, leading to more efficient learning processes.\n",
      "\n",
      "The semiring perspective is particularly valuable as it allows for the generalization of gradient-based methods like backpropagation, enabling the development of novel learning algorithms that are both versatile and computationally efficient. This approach leverages the inherent structure of semirings to optimize neural networks by exploiting cancellation and ordering properties, which can significantly reduce memory usage during backpropagation.\n",
      "\n",
      "A significant contribution of this research is its exploration of tractable models in statistical-relational AI. While model counting on these structures has proven effective for inference tasks, it does not offer the same benefits for learning algorithms like second-order optimization. This limitation highlights a potential gap that could be addressed by further investigation into alternative semiring-based approaches.\n",
      "\n",
      "Empirical evidence from the paper demonstrates that algebraic backpropagation achieves substantial speed improvements compared to existing methods. By integrating semiring properties with model counting, this approach not only enhances computational efficiency but also opens new possibilities for developing more sophisticated and efficient learning algorithms in AI systems. This work contributes to advancing the field by providing a unified framework that bridges traditional inference tasks with modern learning techniques, offering both theoretical insights and practical applications.\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "for paper in papers:\n",
    "    summary = summarize_paper(paper=paper)\n",
    "    print(summary)\n",
    "    print('==========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
